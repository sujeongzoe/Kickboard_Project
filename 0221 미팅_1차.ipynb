{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìœ ì¹˜ì› ë°ì´í„° ìœ„ê²½ë„ ë³€í™˜\n",
    "### ê°•ë‚¨êµ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def doro_to_coords(df):\n",
    "    data = df.copy()\n",
    "    data['ê²½ë„'] = 0\n",
    "    data['ìœ„ë„'] = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if pd.notna(data.loc[i, 'ì£¼ì†Œ']) and data.loc[i, 'ì£¼ì†Œ'] != 'íƒˆí‡´':  # 'ë„ë¡œëª…ì£¼ì†Œ' â†’ 'ì£¼ì†Œ' ë³€ê²½\n",
    "            try:\n",
    "                address = data.loc[i, 'ì£¼ì†Œ']\n",
    "                url = f'https://dapi.kakao.com/v2/local/search/address.json?query={address}'\n",
    "                headers = {\"Authorization\": \"KakaoAK \" + \"39aa05f286da07187ee24e01563aa0f2\"}\n",
    "                response = requests.get(url, headers=headers).json()\n",
    "                \n",
    "                if response.get('documents'):\n",
    "                    temp = response['documents'][0].get('road_address')  # ë„ë¡œëª…ì£¼ì†Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "                    if temp:\n",
    "                        data.loc[i, ['ê²½ë„', 'ìœ„ë„']] = [temp['x'], temp['y']]\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"ì˜¤ë¥˜ ë°œìƒ: {e} (í–‰ {i})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# CSV íŒŒì¼ ì½ê¸° (ê²½ë¡œ ìˆ˜ì •)\n",
    "df = pd.read_csv(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ ìœ ì¹˜ì› ì¼ë°˜í˜„í™©.csv\", encoding='cp949')\n",
    "\n",
    "# í•¨ìˆ˜ ì ìš©\n",
    "result_df = doro_to_coords(df)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ (ê²½ë¡œ ìˆ˜ì •)\n",
    "result_df.to_csv(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ ìœ ì¹˜ì› ì¼ë°˜í˜„í™©_ìœ„ê²½ë„.csv\", encoding='utf-8-sig', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë§ˆí¬êµ¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re  # ğŸ”¥ ì •ê·œì‹ ì‚¬ìš©\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜ í•¨ìˆ˜\n",
    "def doro_to_coords(df):\n",
    "    data = df.copy().reset_index(drop=True)  # ğŸ”¥ ì¸ë±ìŠ¤ ì¬ì„¤ì • ì¶”ê°€\n",
    "    data['ê²½ë„'] = 0\n",
    "    data['ìœ„ë„'] = 0\n",
    "    \n",
    "    # ğŸ”¥ ì£¼ì†Œì—ì„œ ë§ˆì§€ë§‰ ê´„í˜¸ ì œê±°\n",
    "    data['ì£¼ì†Œ'] = data['ì£¼ì†Œ'].apply(lambda x: re.sub(r'\\s*\\(.*?\\)$', '', str(x)) if pd.notna(x) else x)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if pd.notna(data.iloc[i]['ì£¼ì†Œ']) and data.iloc[i]['ì£¼ì†Œ'] != 'íƒˆí‡´':  \n",
    "            try:\n",
    "                address = data.iloc[i]['ì£¼ì†Œ']\n",
    "                url = f'https://dapi.kakao.com/v2/local/search/address.json?query={address}'\n",
    "                headers = {\"Authorization\": \"KakaoAK \" + \"39aa05f286da07187ee24e01563aa0f2\"}\n",
    "                response = requests.get(url, headers=headers).json()\n",
    "                \n",
    "                if response.get('documents'):\n",
    "                    temp = response['documents'][0].get('road_address')  # ë„ë¡œëª…ì£¼ì†Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "                    if temp:\n",
    "                        data.loc[i, ['ê²½ë„', 'ìœ„ë„']] = [temp['x'], temp['y']]\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"ì˜¤ë¥˜ ë°œìƒ: {e} (í–‰ {i})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ğŸ”¹ CSV íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© í™•ì¸ í•„ìš”)\n",
    "file_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸íŠ¹ë³„ì‹œêµìœ¡ì²­ ì„œìš¸íŠ¹ë³„ì‹œì„œë¶€êµìœ¡ì§€ì›ì²­_ê´€ë‚´ í•™êµí˜„í™© ì •ë³´_20240716.csv\"\n",
    "df = pd.read_csv(file_path, encoding='cp949')  \n",
    "\n",
    "# ğŸ”¹ 'í–‰ì •êµ¬ì—­'ì´ 'ë§ˆí¬êµ¬'ì´ê³  'í•™ì œ'ê°€ 'ìœ ì¹˜ì›'ì¸ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "df_filtered = df[(df['í–‰ì •êµ¬ì—­'] == 'ë§ˆí¬êµ¬') & (df['í•™ì œ'] == 'ìœ ì¹˜ì›')].copy()\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜\n",
    "result_df = doro_to_coords(df_filtered)\n",
    "\n",
    "# ğŸ”¹ ë³€í™˜ëœ ë°ì´í„° ì €ì¥\n",
    "output_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ë§ˆí¬êµ¬_ìœ ì¹˜ì›_ìœ„ê²½ë„.csv\"\n",
    "result_df.to_csv(output_path, encoding='utf-8-sig', index=False)\n",
    "\n",
    "print(\"ğŸ‰ ë³€í™˜ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def doro_to_coords(df):\n",
    "    data = df.copy()\n",
    "    data['ê²½ë„'] = 0\n",
    "    data['ìœ„ë„'] = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if pd.notna(data.loc[i, 'ì£¼ì†Œ']) and data.loc[i, 'ì£¼ì†Œ'] != 'íƒˆí‡´':  # 'ë„ë¡œëª…ì£¼ì†Œ' â†’ 'ì£¼ì†Œ' ë³€ê²½\n",
    "            try:\n",
    "                address = data.loc[i, 'ì£¼ì†Œ']\n",
    "                url = f'https://dapi.kakao.com/v2/local/search/address.json?query={address}'\n",
    "                headers = {\"Authorization\": \"KakaoAK \" + \"39aa05f286da07187ee24e01563aa0f2\"}\n",
    "                response = requests.get(url, headers=headers).json()\n",
    "                \n",
    "                if response.get('documents'):\n",
    "                    temp = response['documents'][0].get('road_address')  # ë„ë¡œëª…ì£¼ì†Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "                    if temp:\n",
    "                        data.loc[i, ['ê²½ë„', 'ìœ„ë„']] = [temp['x'], temp['y']]\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"ì˜¤ë¥˜ ë°œìƒ: {e} (í–‰ {i})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# CSV íŒŒì¼ ì½ê¸° (ê²½ë¡œ ìˆ˜ì •)\n",
    "df = pd.read_csv(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ ë§ˆí¬êµ¬ ìœ ì¹˜ì› ì¼ë°˜í˜„í™©.csv\", encoding='cp949')\n",
    "\n",
    "# í•¨ìˆ˜ ì ìš©\n",
    "result_df = doro_to_coords(df)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ (ê²½ë¡œ ìˆ˜ì •)\n",
    "result_df.to_csv(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ ë§ˆí¬êµ¬ ìœ ì¹˜ì› ì¼ë°˜í˜„í™©_ìœ„ê²½ë„.csv\", encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì†¡íŒŒêµ¬ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def doro_to_coords(df):\n",
    "    data = df.copy()\n",
    "    data['ê²½ë„'] = 0\n",
    "    data['ìœ„ë„'] = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if pd.notna(data.loc[i, 'ì£¼ì†Œ']) and data.loc[i, 'ì£¼ì†Œ'] != 'íƒˆí‡´':  # 'ë„ë¡œëª…ì£¼ì†Œ' â†’ 'ì£¼ì†Œ' ë³€ê²½\n",
    "            try:\n",
    "                address = data.loc[i, 'ì£¼ì†Œ']\n",
    "                url = f'https://dapi.kakao.com/v2/local/search/address.json?query={address}'\n",
    "                headers = {\"Authorization\": \"KakaoAK \" + \"39aa05f286da07187ee24e01563aa0f2\"}\n",
    "                response = requests.get(url, headers=headers).json()\n",
    "                \n",
    "                if response.get('documents'):\n",
    "                    temp = response['documents'][0].get('road_address')  # ë„ë¡œëª…ì£¼ì†Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "                    if temp:\n",
    "                        data.loc[i, ['ê²½ë„', 'ìœ„ë„']] = [temp['x'], temp['y']]\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"ì˜¤ë¥˜ ë°œìƒ: {e} (í–‰ {i})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# CSV íŒŒì¼ ì½ê¸° (ê²½ë¡œ ìˆ˜ì •)\n",
    "df = pd.read_csv(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ ì†¡íŒŒêµ¬ ìœ ì¹˜ì› ì¼ë°˜í˜„í™©.csv\", encoding='cp949')\n",
    "\n",
    "# í•¨ìˆ˜ ì ìš©\n",
    "result_df = doro_to_coords(df)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ (ê²½ë¡œ ìˆ˜ì •)\n",
    "result_df.to_csv(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ ì†¡íŒŒêµ¬ ìœ ì¹˜ì› ì¼ë°˜í˜„í™©_ìœ„ê²½ë„.csv\", encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì–´ë¦°ì´ì§‘ ë°ì´í„° ìœ„ê²½ë„ ë³€í™˜\n",
    "ìœ„ê²½ë„ 0ì¸ ë°ì´í„°ê°€ 3 / 5 / 7ê°œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜ í•¨ìˆ˜\n",
    "def doro_to_coords(df):\n",
    "    data = df.copy().reset_index(drop=True)  \n",
    "    data['ê²½ë„'] = 0\n",
    "    data['ìœ„ë„'] = 0\n",
    "    \n",
    "    # ğŸ”¥ ì£¼ì†Œì—ì„œ ë§ˆì§€ë§‰ ê´„í˜¸ ì œê±°\n",
    "    data['ìƒì„¸ì£¼ì†Œ'] = data['ìƒì„¸ì£¼ì†Œ'].apply(lambda x: re.sub(r'\\s*\\(.*?\\)$', '', str(x)) if pd.notna(x) else x)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if pd.notna(data.iloc[i]['ìƒì„¸ì£¼ì†Œ']):  \n",
    "            try:\n",
    "                address = data.iloc[i]['ìƒì„¸ì£¼ì†Œ']\n",
    "                url = f'https://dapi.kakao.com/v2/local/search/address.json?query={address}'\n",
    "                headers = {\"Authorization\": \"KakaoAK \" + \"39aa05f286da07187ee24e01563aa0f2\"}\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response_json = json.loads(response.text.encode('utf-8').decode('utf-8'))\n",
    "\n",
    "                if response_json.get('documents'):\n",
    "                    temp = response_json['documents'][0].get('road_address') or response_json['documents'][0].get('address')\n",
    "                    if temp:\n",
    "                        data.loc[i, ['ê²½ë„', 'ìœ„ë„']] = [temp['x'], temp['y']]\n",
    "                   \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e} (í–‰ {i})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ğŸ”¹ CSV íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© í™•ì¸ í•„ìš”)\n",
    "file_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ ì–´ë¦°ì´ì§‘ ì •ë³´(í‘œì¤€ ë°ì´í„°).csv\"\n",
    "df = pd.read_csv(file_path, encoding='cp949')\n",
    "\n",
    "# ğŸ”¹ 'ì‹œêµ°êµ¬ëª…'ì´ ê°•ë‚¨êµ¬, ë§ˆí¬êµ¬, ì†¡íŒŒêµ¬ì¸ ë°ì´í„° í•„í„°ë§\n",
    "df_filtered = df[df['ì‹œêµ°êµ¬ëª…'].isin(['ê°•ë‚¨êµ¬', 'ë§ˆí¬êµ¬', 'ì†¡íŒŒêµ¬'])].copy()\n",
    "\n",
    "# ğŸ”¹ 'ìš´ì˜í˜„í™©'ì´ \"íì§€\"ê°€ ì•„ë‹Œ ë°ì´í„°ë§Œ ìœ ì§€\n",
    "df_filtered = df_filtered[df_filtered['ìš´ì˜í˜„í™©'] != 'íì§€'].copy()\n",
    "\n",
    "# ğŸ”¹ ì²˜ìŒ 7ê°œ ì¹¼ëŸ¼ + 18, 19ë²ˆì§¸ ì¹¼ëŸ¼ ìœ ì§€\n",
    "cols_to_keep = list(df_filtered.columns[:7]) + [df_filtered.columns[17], df_filtered.columns[18]]\n",
    "df_filtered = df_filtered[cols_to_keep]\n",
    "\n",
    "# ğŸ”¹ ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "missing_values = df_filtered[['ì‹œêµ°êµ¬ëª…', 'ìƒì„¸ì£¼ì†Œ']].isnull().sum()\n",
    "print(\"ğŸ“Œ ê²°ì¸¡ì¹˜ í™•ì¸:\")\n",
    "print(missing_values)\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜\n",
    "result_df = doro_to_coords(df_filtered)\n",
    "\n",
    "# ğŸ”¹ ë³€í™˜ëœ ë°ì´í„° ì €ì¥\n",
    "output_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ê°•ë‚¨_ë§ˆí¬_ì†¡íŒŒ_ì–´ë¦°ì´ì§‘.csv\"\n",
    "result_df.to_csv(output_path, encoding='utf-8-sig', index=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜ í•¨ìˆ˜\n",
    "def doro_to_coords(df):\n",
    "    data = df.copy().reset_index(drop=True)  \n",
    "    data['ê²½ë„'] = 0\n",
    "    data['ìœ„ë„'] = 0\n",
    "    \n",
    "    # ğŸ”¥ ì£¼ì†Œì—ì„œ ë§ˆì§€ë§‰ ê´„í˜¸ ì œê±°\n",
    "    data['ìƒì„¸ì£¼ì†Œ'] = data['ìƒì„¸ì£¼ì†Œ'].apply(lambda x: re.sub(r'\\s*\\(.*?\\)$', '', str(x)) if pd.notna(x) else x)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if pd.notna(data.iloc[i]['ìƒì„¸ì£¼ì†Œ']):  \n",
    "            try:\n",
    "                address = data.iloc[i]['ìƒì„¸ì£¼ì†Œ']\n",
    "                url = f'https://dapi.kakao.com/v2/local/search/address.json?query={address}'\n",
    "                headers = {\"Authorization\": \"KakaoAK \" + \"39aa05f286da07187ee24e01563aa0f2\"}\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response_json = json.loads(response.text.encode('utf-8').decode('utf-8'))\n",
    "\n",
    "                if response_json.get('documents'):\n",
    "                    temp = response_json['documents'][0].get('road_address') or response_json['documents'][0].get('address')\n",
    "                    if temp:\n",
    "                        data.loc[i, ['ê²½ë„', 'ìœ„ë„']] = [temp['x'], temp['y']]\n",
    "                   \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e} (í–‰ {i})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ğŸ”¹ CSV íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© í™•ì¸ í•„ìš”)\n",
    "file_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ ì–´ë¦°ì´ì§‘ ì •ë³´(í‘œì¤€ ë°ì´í„°).csv\"\n",
    "df = pd.read_csv(file_path, encoding='cp949')\n",
    "\n",
    "# ğŸ”¹ 'ì‹œêµ°êµ¬ëª…'ì´ ê°•ë‚¨êµ¬, ë§ˆí¬êµ¬, ì†¡íŒŒêµ¬ì¸ ë°ì´í„° í•„í„°ë§\n",
    "df_filtered = df[df['ì‹œêµ°êµ¬ëª…'].isin(['ê°•ë‚¨êµ¬', 'ë§ˆí¬êµ¬', 'ì†¡íŒŒêµ¬'])].copy()\n",
    "\n",
    "# ğŸ”¹ 'ìš´ì˜í˜„í™©'ì´ \"íì§€\"ê°€ ì•„ë‹Œ ë°ì´í„°ë§Œ ìœ ì§€\n",
    "df_filtered = df_filtered[df_filtered['ìš´ì˜í˜„í™©'] != 'íì§€'].copy()\n",
    "\n",
    "# ğŸ”¹ ì²˜ìŒ 7ê°œ ì¹¼ëŸ¼ + 18, 19ë²ˆì§¸ ì¹¼ëŸ¼ ìœ ì§€\n",
    "cols_to_keep = list(df_filtered.columns[:7]) + [df_filtered.columns[17], df_filtered.columns[18]]\n",
    "df_filtered = df_filtered[cols_to_keep]\n",
    "\n",
    "# ğŸ”¹ ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "missing_values = df_filtered[['ì‹œêµ°êµ¬ëª…', 'ìƒì„¸ì£¼ì†Œ']].isnull().sum()\n",
    "print(\"ğŸ“Œ ê²°ì¸¡ì¹˜ í™•ì¸:\")\n",
    "print(missing_values)\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜\n",
    "result_df = doro_to_coords(df_filtered)\n",
    "\n",
    "# ğŸ”¹ ê°•ë‚¨êµ¬, ë§ˆí¬êµ¬, ì†¡íŒŒêµ¬ ê°ê° ì €ì¥ ë° ìœ„ê²½ë„ ê°’ì´ 0ì¸ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
    "for region in ['ê°•ë‚¨êµ¬', 'ë§ˆí¬êµ¬', 'ì†¡íŒŒêµ¬']:\n",
    "    df_region = result_df[result_df['ì‹œêµ°êµ¬ëª…'] == region]\n",
    "    \n",
    "    # ë³€í™˜ ì‹¤íŒ¨í•œ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
    "    zero_coords_count = ((df_region['ê²½ë„'] == 0) & (df_region['ìœ„ë„'] == 0)).sum()\n",
    "    print(zero_coords_count)\n",
    "    # CSV ì €ì¥\n",
    "    output_path = fr\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\{region}_ì–´ë¦°ì´ì§‘.csv\"\n",
    "    df_region.to_csv(output_path, encoding='utf-8-sig', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ê²½ë¡œë‹¹ ë°ì´í„° ìœ„ê²½ë„ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜ í•¨ìˆ˜\n",
    "def doro_to_coords(df):\n",
    "    data = df.copy().reset_index(drop=True)  \n",
    "    data['ê²½ë„'] = 0\n",
    "    data['ìœ„ë„'] = 0\n",
    "    \n",
    "    # ğŸ”¥ ì£¼ì†Œì—ì„œ ë§ˆì§€ë§‰ ê´„í˜¸ ì œê±° & \"ì„œìš¸íŠ¹ë³„ì‹œ [ìì¹˜êµ¬ëª…] \" ì¶”ê°€\n",
    "    data['ì£¼ì†Œ'] = data.apply(lambda row: f\"ì„œìš¸íŠ¹ë³„ì‹œ {row['ìì¹˜êµ¬']} \" + re.sub(r'\\s*\\(.*?\\)$', '', str(row['Unnamed: 3'])) if pd.notna(row['Unnamed: 3']) else row['Unnamed: 3'], axis=1)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if pd.notna(data.iloc[i]['ì£¼ì†Œ']):  \n",
    "            try:\n",
    "                address = data.iloc[i]['ì£¼ì†Œ']\n",
    "                url = f'https://dapi.kakao.com/v2/local/search/address.json?query={address}'\n",
    "                headers = {\"Authorization\": \"KakaoAK \" + \"39aa05f286da07187ee24e01563aa0f2\"}\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response_json = json.loads(response.text.encode('utf-8').decode('utf-8'))\n",
    "\n",
    "                if response_json.get('documents'):\n",
    "                    temp = response_json['documents'][0].get('road_address') or response_json['documents'][0].get('address')\n",
    "                    if temp:\n",
    "                        data.loc[i, ['ê²½ë„', 'ìœ„ë„']] = [temp['x'], temp['y']]\n",
    "                   \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e} (í–‰ {i})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ğŸ”¹ ì—‘ì…€ íŒŒì¼ ì½ê¸°\n",
    "file_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì—´ë¦°ë°ì´í„°V2(ê²½ë¡œë‹¹í˜„í™©-ìµœì‹ ìë£Œ3596ê°œì†Œ)_2401.xlsx\"\n",
    "\n",
    "# ğŸ”¹ ê°•ë‚¨êµ¬, ë§ˆí¬êµ¬, ì†¡íŒŒêµ¬ ì‹œíŠ¸ë§Œ ì¶”ì¶œ\n",
    "sheets_to_extract = ['ê°•ë‚¨êµ¬', 'ë§ˆí¬êµ¬', 'ì†¡íŒŒêµ¬']\n",
    "dfs = pd.read_excel(file_path, sheet_name=sheets_to_extract)\n",
    "\n",
    "# ğŸ”¹ ê° ì‹œíŠ¸ë³„ ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥\n",
    "for region, df in dfs.items():\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ğŸ”¹ ì»¬ëŸ¼ëª… í‘œì¤€í™”\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # ğŸ”¹ 'ìì¹˜êµ¬' ì¹¼ëŸ¼ ì¶”ê°€ (ì‹œíŠ¸ëª… ê¸°ì¤€)\n",
    "    df['ìì¹˜êµ¬'] = region\n",
    "    \n",
    "    # ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜\n",
    "    df_result = doro_to_coords(df)\n",
    "    \n",
    "    # ğŸ”¹ ë³€í™˜ëœ ë°ì´í„° ì €ì¥\n",
    "    output_path = fr\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\{region}_ê²½ë¡œë‹¹.csv\"\n",
    "    df_result.to_csv(output_path, encoding='utf-8-sig', index=False)\n",
    "    \n",
    "    # ğŸ”¹ ë³€í™˜ ì‹¤íŒ¨í•œ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
    "    zero_coords_count = ((df_result['ê²½ë„'] == 0) & (df_result['ìœ„ë„'] == 0)).sum()\n",
    "    \n",
    "    print(f\"ğŸ“‚ {region} ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "    print(f\"âš ï¸ {region}ì—ì„œ ìœ„ê²½ë„ ë³€í™˜ ì‹¤íŒ¨í•œ ë°ì´í„° ê°œìˆ˜: {zero_coords_count}ê°œ\\n\")\n",
    "\n",
    "print(\"ğŸ‰ ëª¨ë“  ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì–´ë¦°ì´ / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™êµ(ì´ˆ,ì¤‘,ê³ ) ë°ì´í„°ì…‹ ë³€í™˜í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë„ë¡œëª…ì£¼ì†Œ(7ë²ˆì§¸ ì»¬ëŸ¼)ì— ê²°ì¸¡ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>í•™êµì¢…ë¥˜ëª…</td>\n",
       "      <td>ì„¤ë¦½êµ¬ë¶„</td>\n",
       "      <td>í‘œì¤€í•™êµì½”ë“œ</td>\n",
       "      <td>í•™êµëª…</td>\n",
       "      <td>ì˜ë¬¸í•™êµëª…</td>\n",
       "      <td>ê´€í• ì¡°ì§ëª…</td>\n",
       "      <td>ë„ë¡œëª…ìš°í¸ë²ˆí˜¸</td>\n",
       "      <td>ë„ë¡œëª…ì£¼ì†Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ê°ì¢…í•™êµ(ì¤‘)</td>\n",
       "      <td>ì‚¬ë¦½</td>\n",
       "      <td>7134155</td>\n",
       "      <td>ì„ í™”ì˜ˆìˆ ì¤‘í•™êµ</td>\n",
       "      <td>Sunhwa Arts Middle School</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œì„±ë™ê´‘ì§„êµìœ¡ì§€ì›ì²­</td>\n",
       "      <td>04991</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ê´‘ì§„êµ¬ ì²œí˜¸ëŒ€ë¡œ 664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì´ˆë“±í•™êµ</td>\n",
       "      <td>ê³µë¦½</td>\n",
       "      <td>7134150</td>\n",
       "      <td>ì„œìš¸ìˆ­ì‹ ì´ˆë“±í•™êµ</td>\n",
       "      <td>Seoul Soongshin Elementary School</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œì„±ë™ê´‘ì§„êµìœ¡ì§€ì›ì²­</td>\n",
       "      <td>04702</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë™êµ¬ ë§ˆì¥ë¡œ 161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì¤‘í•™êµ</td>\n",
       "      <td>ê³µë¦½</td>\n",
       "      <td>7134142</td>\n",
       "      <td>í–‰ë‹¹ì¤‘í•™êµ</td>\n",
       "      <td>Haengdang Middle School</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œì„±ë™ê´‘ì§„êµìœ¡ì§€ì›ì²­</td>\n",
       "      <td>04764</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë™êµ¬ ì™•ì‹­ë¦¬ë¡œ 189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì¤‘í•™êµ</td>\n",
       "      <td>ì‚¬ë¦½</td>\n",
       "      <td>7134141</td>\n",
       "      <td>í•œì–‘ëŒ€í•™êµì‚¬ë²”ëŒ€í•™ë¶€ì†ì¤‘í•™êµ</td>\n",
       "      <td>Hanyang University Middle School</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œì„±ë™ê´‘ì§„êµìœ¡ì§€ì›ì²­</td>\n",
       "      <td>04761</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë™êµ¬ ë§ˆì¡°ë¡œ 42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3924</th>\n",
       "      <td>ê³ ë“±í•™êµ</td>\n",
       "      <td>êµ­ë¦½</td>\n",
       "      <td>1371661</td>\n",
       "      <td>êµ­ë¦½êµ­ì•…ê³ ë“±í•™êµ</td>\n",
       "      <td>Gugak National High School</td>\n",
       "      <td>êµìœ¡ë¶€</td>\n",
       "      <td>06311</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ê°œí¬ë¡œ22ê¸¸ 65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>íŠ¹ìˆ˜í•™êµ</td>\n",
       "      <td>êµ­ë¦½</td>\n",
       "      <td>1342102</td>\n",
       "      <td>í•œêµ­ìš°ì§„í•™êµ</td>\n",
       "      <td>Hanguk Woojin School</td>\n",
       "      <td>êµìœ¡ë¶€</td>\n",
       "      <td>03934</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ë§ˆí¬êµ¬ ì›”ë“œì»µë¶ë¡œ38ê¸¸ 21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3926</th>\n",
       "      <td>íŠ¹ìˆ˜í•™êµ</td>\n",
       "      <td>êµ­ë¦½</td>\n",
       "      <td>1342099</td>\n",
       "      <td>ì„œìš¸ë†í•™êµ</td>\n",
       "      <td>Seoul National school for the Deaf</td>\n",
       "      <td>êµìœ¡ë¶€</td>\n",
       "      <td>03032</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ í•„ìš´ëŒ€ë¡œ 103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3927</th>\n",
       "      <td>íŠ¹ìˆ˜í•™êµ</td>\n",
       "      <td>êµ­ë¦½</td>\n",
       "      <td>1342098</td>\n",
       "      <td>ì„œìš¸ë§¹í•™êµ</td>\n",
       "      <td>Seoul National School for the Blind</td>\n",
       "      <td>êµìœ¡ë¶€</td>\n",
       "      <td>03032</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ í•„ìš´ëŒ€ë¡œ 97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>ê³µë™ì‹¤ìŠµì†Œ</td>\n",
       "      <td>ê³µë¦½</td>\n",
       "      <td>0000000</td>\n",
       "      <td>ê²½ê¸°ê¸°ê³„ê³µì—…ê³ ë“±í•™êµë¶€ì„¤ë¯¸ë˜ê¸°ìˆ êµìœ¡ì„¼í„°</td>\n",
       "      <td>.</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œêµìœ¡ì²­</td>\n",
       "      <td>01810</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œ ë…¸ì›êµ¬ ê³µë¦‰ë¡œ 264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3929 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0     1        2                     3  \\\n",
       "0       í•™êµì¢…ë¥˜ëª…  ì„¤ë¦½êµ¬ë¶„   í‘œì¤€í•™êµì½”ë“œ                   í•™êµëª…   \n",
       "1     ê°ì¢…í•™êµ(ì¤‘)    ì‚¬ë¦½  7134155               ì„ í™”ì˜ˆìˆ ì¤‘í•™êµ   \n",
       "2        ì´ˆë“±í•™êµ    ê³µë¦½  7134150              ì„œìš¸ìˆ­ì‹ ì´ˆë“±í•™êµ   \n",
       "3         ì¤‘í•™êµ    ê³µë¦½  7134142                 í–‰ë‹¹ì¤‘í•™êµ   \n",
       "4         ì¤‘í•™êµ    ì‚¬ë¦½  7134141        í•œì–‘ëŒ€í•™êµì‚¬ë²”ëŒ€í•™ë¶€ì†ì¤‘í•™êµ   \n",
       "...       ...   ...      ...                   ...   \n",
       "3924     ê³ ë“±í•™êµ    êµ­ë¦½  1371661              êµ­ë¦½êµ­ì•…ê³ ë“±í•™êµ   \n",
       "3925     íŠ¹ìˆ˜í•™êµ    êµ­ë¦½  1342102                í•œêµ­ìš°ì§„í•™êµ   \n",
       "3926     íŠ¹ìˆ˜í•™êµ    êµ­ë¦½  1342099                 ì„œìš¸ë†í•™êµ   \n",
       "3927     íŠ¹ìˆ˜í•™êµ    êµ­ë¦½  1342098                 ì„œìš¸ë§¹í•™êµ   \n",
       "3928    ê³µë™ì‹¤ìŠµì†Œ    ê³µë¦½  0000000  ê²½ê¸°ê¸°ê³„ê³µì—…ê³ ë“±í•™êµë¶€ì„¤ë¯¸ë˜ê¸°ìˆ êµìœ¡ì„¼í„°   \n",
       "\n",
       "                                        4               5        6  \\\n",
       "0                                   ì˜ë¬¸í•™êµëª…           ê´€í• ì¡°ì§ëª…  ë„ë¡œëª…ìš°í¸ë²ˆí˜¸   \n",
       "1               Sunhwa Arts Middle School  ì„œìš¸íŠ¹ë³„ì‹œì„±ë™ê´‘ì§„êµìœ¡ì§€ì›ì²­    04991   \n",
       "2       Seoul Soongshin Elementary School  ì„œìš¸íŠ¹ë³„ì‹œì„±ë™ê´‘ì§„êµìœ¡ì§€ì›ì²­    04702   \n",
       "3                 Haengdang Middle School  ì„œìš¸íŠ¹ë³„ì‹œì„±ë™ê´‘ì§„êµìœ¡ì§€ì›ì²­    04764   \n",
       "4        Hanyang University Middle School  ì„œìš¸íŠ¹ë³„ì‹œì„±ë™ê´‘ì§„êµìœ¡ì§€ì›ì²­    04761   \n",
       "...                                   ...             ...      ...   \n",
       "3924           Gugak National High School             êµìœ¡ë¶€    06311   \n",
       "3925                 Hanguk Woojin School             êµìœ¡ë¶€    03934   \n",
       "3926   Seoul National school for the Deaf             êµìœ¡ë¶€    03032   \n",
       "3927  Seoul National School for the Blind             êµìœ¡ë¶€    03032   \n",
       "3928                                    .        ì„œìš¸íŠ¹ë³„ì‹œêµìœ¡ì²­    01810   \n",
       "\n",
       "                          7  \n",
       "0                     ë„ë¡œëª…ì£¼ì†Œ  \n",
       "1        ì„œìš¸íŠ¹ë³„ì‹œ ê´‘ì§„êµ¬ ì²œí˜¸ëŒ€ë¡œ 664  \n",
       "2         ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë™êµ¬ ë§ˆì¥ë¡œ 161  \n",
       "3        ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë™êµ¬ ì™•ì‹­ë¦¬ë¡œ 189  \n",
       "4          ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë™êµ¬ ë§ˆì¡°ë¡œ 42  \n",
       "...                     ...  \n",
       "3924    ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ê°œí¬ë¡œ22ê¸¸ 65  \n",
       "3925  ì„œìš¸íŠ¹ë³„ì‹œ ë§ˆí¬êµ¬ ì›”ë“œì»µë¶ë¡œ38ê¸¸ 21  \n",
       "3926     ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ í•„ìš´ëŒ€ë¡œ 103  \n",
       "3927      ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ í•„ìš´ëŒ€ë¡œ 97  \n",
       "3928      ì„œìš¸íŠ¹ë³„ì‹œ ë…¸ì›êµ¬ ê³µë¦‰ë¡œ 264  \n",
       "\n",
       "[3929 rows x 8 columns]"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ”¹ íŒŒì¼ ì½ê¸° (ì „ì²´ í–‰ì„ ì½ê³  ì²« 8ê°œ ì¹¼ëŸ¼ë§Œ ì‚¬ìš©)\n",
    "file_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ í•™êµ ê¸°ë³¸ì •ë³´.csv\"\n",
    "df2 = pd.read_csv(file_path, encoding='cp949', header=None)\n",
    "\n",
    "# ğŸ”¹ ì²˜ìŒ 8ê°œì˜ ì¹¼ëŸ¼ë§Œ ì„ íƒ\n",
    "df2 = df2.iloc[:, :8]\n",
    "\n",
    "# ğŸ”¹ ë„ë¡œëª…ì£¼ì†Œ ì»¬ëŸ¼ì— ê²°ì¸¡ì¹˜ ìˆëŠ”ì§€ í™•ì¸\n",
    "missing_addresses = df2[7].isna().sum()  # 7ë²ˆì§¸ ì»¬ëŸ¼(ë„ë¡œëª…ì£¼ì†Œ)ì˜ ê²°ì¸¡ì¹˜ ê°œìˆ˜\n",
    "if missing_addresses > 0:\n",
    "    print(f\"âš ï¸ ë„ë¡œëª…ì£¼ì†Œ(7ë²ˆì§¸ ì»¬ëŸ¼)ì— ê²°ì¸¡ì¹˜ê°€ {missing_addresses}ê°œ ìˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"ë„ë¡œëª…ì£¼ì†Œ(7ë²ˆì§¸ ì»¬ëŸ¼)ì— ê²°ì¸¡ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc7 in position 1: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[490], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ğŸ”¹ íŒŒì¼ ì½ê¸° (ì „ì²´ í–‰ì„ ì½ê³  ì²« 8ê°œ ì¹¼ëŸ¼ë§Œ ì‚¬ìš©)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m0221 ë¯¸íŒ…\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mì„œìš¸ì‹œ í•™êµ ê¸°ë³¸ì •ë³´.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 32\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8-sig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# ğŸ”¹ ì²˜ìŒ 8ê°œì˜ ì¹¼ëŸ¼ë§Œ ì„ íƒ\u001b[39;00m\n\u001b[0;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m8\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:782\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\utf_8_sig.py:69\u001b[0m, in \u001b[0;36mIncrementalDecoder._buffer_decode\u001b[1;34m(self, input, errors, final)\u001b[0m\n\u001b[0;32m     66\u001b[0m             (output, consumed) \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m     67\u001b[0m                codecs\u001b[38;5;241m.\u001b[39mutf_8_decode(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m3\u001b[39m:], errors, final)\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m (output, consumed\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutf_8_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc7 in position 1: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜ í•¨ìˆ˜\n",
    "def doro_to_coords(df):\n",
    "    data = df.copy().reset_index(drop=True)\n",
    "    data['ê²½ë„'] = 0\n",
    "    data['ìœ„ë„'] = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if pd.notna(data.iloc[i][7]):  # ë„ë¡œëª…ì£¼ì†Œê°€ ì¡´ì¬í•  ë•Œë§Œ ë³€í™˜ ì‹œë„ (7ë²ˆì§¸ ì»¬ëŸ¼ì´ ë„ë¡œëª…ì£¼ì†Œ)\n",
    "            try:\n",
    "                address = data.iloc[i][7]  # 7ë²ˆì§¸ ì»¬ëŸ¼(ë„ë¡œëª…ì£¼ì†Œ)\n",
    "                url = f'https://dapi.kakao.com/v2/local/search/address.json?query={address}'\n",
    "                headers = {\"Authorization\": \"KakaoAK \" + \"39aa05f286da07187ee24e01563aa0f2\"}\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response_json = json.loads(response.text.encode('utf-8').decode('utf-8'))\n",
    "\n",
    "                if response_json.get('documents'):\n",
    "                    temp = response_json['documents'][0].get('road_address') or response_json['documents'][0].get('address')\n",
    "                    if temp:\n",
    "                        data.loc[i, ['ê²½ë„', 'ìœ„ë„']] = [temp['x'], temp['y']]\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e} (í–‰ {i})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ğŸ”¹ íŒŒì¼ ì½ê¸° (ì „ì²´ í–‰ì„ ì½ê³  ì²« 8ê°œ ì¹¼ëŸ¼ë§Œ ì‚¬ìš©)\n",
    "file_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ í•™êµ ê¸°ë³¸ì •ë³´.csv\"\n",
    "df = pd.read_csv(file_path, encoding='cp949', header=None)\n",
    "\n",
    "# ğŸ”¹ ì²˜ìŒ 8ê°œì˜ ì¹¼ëŸ¼ë§Œ ì„ íƒ\n",
    "df = df.iloc[:, :8]\n",
    "\n",
    "# ğŸ”¹ ë„ë¡œëª…ì£¼ì†Œì—ì„œ ìì¹˜êµ¬ ì¶”ì¶œí•˜ê¸° (ê°•ë‚¨êµ¬, ë§ˆí¬êµ¬, ì†¡íŒŒêµ¬)\n",
    "df['ìì¹˜êµ¬'] = df[7].apply(lambda x: 'ê°•ë‚¨êµ¬' if 'ê°•ë‚¨êµ¬' in str(x) \n",
    "                                     else ('ë§ˆí¬êµ¬' if 'ë§ˆí¬êµ¬' in str(x) \n",
    "                                           else ('ì†¡íŒŒêµ¬' if 'ì†¡íŒŒêµ¬' in str(x) else None)))\n",
    "\n",
    "# ğŸ”¹ ìì¹˜êµ¬ë³„ë¡œ í•„í„°ë§\n",
    "df_filtered = df[df['ìì¹˜êµ¬'].isin(['ê°•ë‚¨êµ¬', 'ë§ˆí¬êµ¬', 'ì†¡íŒŒêµ¬'])].copy()\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜\n",
    "df_result = doro_to_coords(df_filtered)\n",
    "\n",
    "# ğŸ”¹ ìœ„ê²½ë„ ë³€í™˜ ì‹¤íŒ¨í•œ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
    "zero_coords_count = ((df_result['ê²½ë„'] == 0) & (df_result['ìœ„ë„'] == 0)).sum()\n",
    "\n",
    "# ğŸ”¹ ê° êµ¬ë³„ë¡œ ì¤‘ë³µëœ í•™êµ ì œê±°\n",
    "for region in ['ê°•ë‚¨êµ¬', 'ë§ˆí¬êµ¬', 'ì†¡íŒŒêµ¬']:\n",
    "    region_df = df_result[df_result['ìì¹˜êµ¬'] == region]\n",
    "    # ë„ë¡œëª…ì£¼ì†Œ(7ë²ˆì§¸ ì»¬ëŸ¼) ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "    region_df = region_df.drop_duplicates(subset=[3])  \n",
    "    region_df.to_csv(f\"C:\\\\Users\\\\user\\\\Desktop\\\\0221 ë¯¸íŒ…\\\\{region}_í•™êµì •ë³´.csv\", encoding='utf-8-sig', index=False)\n",
    "    print(f\"ğŸ“‚ {region} ë°ì´í„° ì €ì¥ ì™„ë£Œ\")\n",
    "    \n",
    "print(f\"âš ï¸ ìœ„ê²½ë„ ë³€í™˜ ì‹¤íŒ¨í•œ ë°ì´í„° ê°œìˆ˜: {zero_coords_count}ê°œ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„œìš¸ì‹œ ì‚¬íšŒë³µì§€ì‹œì„¤ ë°ì´í„° ìœ„ê²½ë„ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜ í•¨ìˆ˜\n",
    "def doro_to_coords(df):\n",
    "    data = df.copy().reset_index(drop=True)\n",
    "    data['ê²½ë„'] = 0\n",
    "    data['ìœ„ë„'] = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if pd.notna(data.iloc[i]['ì‹œì„¤ì£¼ì†Œ']):  # ì‹œì„¤ì£¼ì†Œê°€ ì¡´ì¬í•  ë•Œë§Œ ë³€í™˜ ì‹œë„\n",
    "            try:\n",
    "                address = data.iloc[i]['ì‹œì„¤ì£¼ì†Œ']\n",
    "                url = f'https://dapi.kakao.com/v2/local/search/address.json?query={address}'\n",
    "                headers = {\"Authorization\": \"KakaoAK \" + \"39aa05f286da07187ee24e01563aa0f2\"}\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response_json = json.loads(response.text.encode('utf-8').decode('utf-8'))\n",
    "\n",
    "                if response_json.get('documents'):\n",
    "                    temp = response_json['documents'][0].get('road_address') or response_json['documents'][0].get('address')\n",
    "                    if temp:\n",
    "                        data.loc[i, ['ê²½ë„', 'ìœ„ë„']] = [temp['x'], temp['y']]\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e} (í–‰ {i})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ğŸ”¹ íŒŒì¼ ì½ê¸°\n",
    "file_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ ì‚¬íšŒë³µì§€ì‹œì„¤ ëª©ë¡.csv\"\n",
    "df = pd.read_csv(file_path, encoding='cp949')\n",
    "\n",
    "# ğŸ”¹ ì‹œêµ°êµ¬ëª… & ì‹œì„¤ì£¼ì†Œ ì¹¼ëŸ¼ ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "missing_sgg = df['ì‹œêµ°êµ¬ëª…'].isna().sum()\n",
    "missing_address = df['ì‹œì„¤ì£¼ì†Œ'].isna().sum()\n",
    "print(f\"âš ï¸ ì‹œêµ°êµ¬ëª… ê²°ì¸¡ì¹˜ ê°œìˆ˜: {missing_sgg}ê°œ\")\n",
    "print(f\"âš ï¸ ì‹œì„¤ì£¼ì†Œ ê²°ì¸¡ì¹˜ ê°œìˆ˜: {missing_address}ê°œ\")\n",
    "\n",
    "# ğŸ”¹ ì‹œì„¤ì£¼ì†Œê°€ ê²°ì¸¡ì¹˜ì´ê³ , ì‹œêµ°êµ¬ëª…ì´ ê°•ë‚¨êµ¬/ë§ˆí¬êµ¬/ì†¡íŒŒêµ¬ì¸ ë°ì´í„° í™•ì¸\n",
    "missing_address_filtered = df[df['ì‹œì„¤ì£¼ì†Œ'].isna() & df['ì‹œêµ°êµ¬ëª…'].isin(['ê°•ë‚¨êµ¬', 'ë§ˆí¬êµ¬', 'ì†¡íŒŒêµ¬'])]\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ ì¶œë ¥\n",
    "if not missing_address_filtered.empty:\n",
    "    print(\"âš ï¸ ì‹œì„¤ì£¼ì†Œê°€ ê²°ì¸¡ì¹˜ì´ê³ , ì‹œêµ°êµ¬ëª…ì´ ê°•ë‚¨êµ¬/ë§ˆí¬êµ¬/ì†¡íŒŒêµ¬ì¸ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤!\")\n",
    "    print(missing_address_filtered)\n",
    "else:\n",
    "    print(\"âœ… ì‹œì„¤ì£¼ì†Œê°€ ê²°ì¸¡ì¹˜ì¸ ë°ì´í„° ì¤‘ ê°•ë‚¨êµ¬, ë§ˆí¬êµ¬, ì†¡íŒŒêµ¬ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ëŠ” ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ğŸ”¹ ê°•ë‚¨êµ¬, ë§ˆí¬êµ¬, ì†¡íŒŒêµ¬ ë°ì´í„° í•„í„°ë§\n",
    "df_filtered = df[df['ì‹œêµ°êµ¬ëª…'].isin(['ê°•ë‚¨êµ¬', 'ë§ˆí¬êµ¬', 'ì†¡íŒŒêµ¬'])].copy()\n",
    "\n",
    "# ğŸ”¹ ì‹œì„¤ì£¼ì†Œì—ì„œ ê´„í˜¸ ì œê±°\n",
    "df_filtered['ì‹œì„¤ì£¼ì†Œ'] = df_filtered['ì‹œì„¤ì£¼ì†Œ'].apply(lambda x: re.sub(r'\\s*\\(.*?\\)$', '', str(x)) if pd.notna(x) else x)\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ â†’ ê²½ë„, ìœ„ë„ ë³€í™˜\n",
    "df_result = doro_to_coords(df_filtered)\n",
    "\n",
    "# ğŸ”¹ ìœ„ê²½ë„ ë³€í™˜ ì‹¤íŒ¨í•œ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
    "zero_coords_count = ((df_result['ê²½ë„'] == 0) & (df_result['ìœ„ë„'] == 0)).sum()\n",
    "\n",
    "# ğŸ”¹ ê°•ë‚¨êµ¬, ë§ˆí¬êµ¬, ì†¡íŒŒêµ¬ë³„ë¡œ ë°ì´í„° ì €ì¥\n",
    "for region in ['ê°•ë‚¨êµ¬', 'ë§ˆí¬êµ¬', 'ì†¡íŒŒêµ¬']:\n",
    "    region_df = df_result[df_result['ì‹œêµ°êµ¬ëª…'] == region]\n",
    "    region_df.to_csv(f\"C:\\\\Users\\\\user\\\\Desktop\\\\0221 ë¯¸íŒ…\\\\{region}_ì‚¬íšŒë³µì§€ì‹œì„¤.csv\", encoding='utf-8-sig', index=False)\n",
    "    print(f\"ğŸ“‚ {region} ë°ì´í„° ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "print(f\"âš ï¸ ìœ„ê²½ë„ ë³€í™˜ ì‹¤íŒ¨í•œ ë°ì´í„° ê°œìˆ˜: {zero_coords_count}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¹ ìœ„ê²½ë„ê°€ 0ì¸ ë°ì´í„° ê°œìˆ˜ & ì´ ë°ì´í„° ê°œìˆ˜ & ë¹„ìœ¨ ê³„ì‚°\n",
    "summary = {}\n",
    "\n",
    "for region in ['ê°•ë‚¨êµ¬', 'ë§ˆí¬êµ¬', 'ì†¡íŒŒêµ¬']:\n",
    "    total_count = df_result[df_result['ì‹œêµ°êµ¬ëª…'] == region].shape[0]  # í•´ë‹¹ êµ¬ì˜ ì´ ë°ì´í„° ê°œìˆ˜\n",
    "    zero_coords_count = df_result[(df_result['ì‹œêµ°êµ¬ëª…'] == region) & (df_result['ìœ„ë„'] == 0) & (df_result['ê²½ë„'] == 0)].shape[0]  # ìœ„ê²½ë„ê°€ 0ì¸ ê°œìˆ˜\n",
    "    zero_coords_ratio = (zero_coords_count / total_count) * 100 if total_count > 0 else 0  # ë¹„ìœ¨ (%)\n",
    "\n",
    "    summary[region] = {\n",
    "        \"ì´ ë°ì´í„° ê°œìˆ˜\": total_count,\n",
    "        \"ìœ„ê²½ë„ 0 ê°œìˆ˜\": zero_coords_count,\n",
    "        \"ë¹„ìœ¨ (%)\": round(zero_coords_ratio, 2)\n",
    "    }\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ ì¶œë ¥\n",
    "for region, stats in summary.items():\n",
    "    print(f\"ğŸ“ {region}\")\n",
    "    print(f\"   â–¶ ì´ ë°ì´í„° ê°œìˆ˜: {stats['ì´ ë°ì´í„° ê°œìˆ˜']}ê°œ\")\n",
    "    print(f\"   â–¶ ìœ„ê²½ë„ 0 ê°œìˆ˜: {stats['ìœ„ê²½ë„ 0 ê°œìˆ˜']}ê°œ\")\n",
    "    print(f\"   â–¶ ë¹„ìœ¨: {stats['ë¹„ìœ¨ (%)']}%\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„œìš¸ì‹œ í†µí–‰ë¶ˆí¸ì§€ì—­ ë°ì´í„°ì…‹ ìœ„ê²½ë„ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://openapi.seoul.go.kr:8088/6e74564a6272756238377341677154/json/passWeak/1/1000/'\n",
    "\n",
    "res = requests.get(url)\n",
    "data = res.json()\n",
    "\n",
    "# ì´ ë°ì´í„° ê°œìˆ˜ í™•ì¸ (APIì—ì„œ ì œê³µí•˜ëŠ” ì´ ê°œìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•)\n",
    "total_count = data['passWeak']['list_total_count']\n",
    "print(f\"ì´ ë°ì´í„° ê°œìˆ˜: {total_count}\") # 13877ê°œ\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ë¹ˆ ë¦¬ìŠ¤íŠ¸\n",
    "all_data = []\n",
    "\n",
    "# 1000ê°œì”© ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "for start in range(1, 14000, 1000):\n",
    "    url = f'http://openapi.seoul.go.kr:8088/45596e627172616435346f7a486f4e/json/passWeak/{start}/{start+999}/'\n",
    "    content = requests.get(url).json()\n",
    "    all_data.extend(content.get('passWeak', {}).get('row', []))\n",
    "\n",
    "# í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "result = pd.DataFrame(all_data)\n",
    "\n",
    "# pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd\n",
    "result.to_excel(r\"C:\\\\Users\\\\user\\\\Desktop\\\\0221 ë¯¸íŒ…\\\\ì„œìš¸ì‹œí†µí–‰ë¶ˆí¸ì§€ì—­.xlsx\", index=False)\n",
    "\n",
    "result = pd.read_excel(r\"C:\\\\Users\\\\user\\\\Desktop\\\\0221 ë¯¸íŒ…\\\\ì„œìš¸ì‹œí†µí–‰ë¶ˆí¸ì§€ì—­.xlsx\")\n",
    "\n",
    "# ì¹¼ëŸ¼ëª… ë³€ê²½\n",
    "result.columns = [\n",
    "    'ì¼ë ¨ë²ˆí˜¸',            # ì¼ë ¨ë²ˆí˜¸\n",
    "    'í†µí–‰ë¶ˆí¸ì§€ì—­ì´ë¦„',   # í†µí–‰ë¶ˆí¸ì§€ì—­ ì´ë¦„\n",
    "    'êµ¬ë¶„ì½”ë“œ',            # êµ¬ë¶„ì½”ë“œ\n",
    "    'ì¡°ì‚¬ì¼ì',            # ì¡°ì‚¬ì¼ì\n",
    "    'ë¹„ê³ ',                # ë¹„ê³ \n",
    "    'ì‹œêµ°êµ¬ì½”ë“œ'  ]         # ì‹œêµ°êµ¬ì½”ë“œ\n",
    "\n",
    "# ë³€ê²½ëœ ë°ì´í„°í”„ë ˆì„ í™•ì¸\n",
    "print(result.head())\n",
    "print()\n",
    "\n",
    "# 'ì‹œêµ°êµ¬ì½”ë“œ'ê°€ '11'ë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í™•ì¸\n",
    "non_11_sgg_cd = result[~result['ì‹œêµ°êµ¬ì½”ë“œ'].astype(str).str.startswith('11')]\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(non_11_sgg_cd) # ì—†ìŒ\n",
    "\n",
    "# ê²°ì¸¡ê°’ í™•ì¸ => ê²°ì¸¡ì¹˜ ì—†ìŒ\n",
    "missing_values = result.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ğŸ”¹ ì—‘ì…€ íŒŒì¼ ì½ê¸°\n",
    "file_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œí†µí–‰ë¶ˆí¸ì§€ì—­.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# ğŸ”¹ ì£¼ì†Œ ì •ì œ í•¨ìˆ˜ (ìˆ«ì í¬í•¨ëœ ì£¼ì†Œê¹Œì§€ë§Œ ì¶”ì¶œ)\n",
    "def clean_address(address):\n",
    "    match = re.search(r'^[^\\d]*\\d+[-\\d]*', str(address))  # ì²« ë²ˆì§¸ ìˆ«ìê°€ ë‚˜ì˜¤ëŠ” ë¶€ë¶„ê¹Œì§€ë§Œ ì¶”ì¶œ\n",
    "    return match.group() if match else address  # ë§¤ì¹­ëœ ë¶€ë¶„ ë°˜í™˜, ì—†ìœ¼ë©´ ì›ë˜ ì£¼ì†Œ ë°˜í™˜\n",
    "\n",
    "# ğŸ”¹ ì •ì œëœ ì£¼ì†Œ ì¹¼ëŸ¼ ì¶”ê°€\n",
    "df['ì •ì œëœì£¼ì†Œ'] = df['PSRD_ICVNC_RGN_NM'].apply(clean_address)\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ ì €ì¥ (encoding ì˜µì…˜ ì œê±°)\n",
    "output_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œí†µí–‰ë¶ˆí¸ì§€ì—­_ì •ì œ.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ í™•ì¸\n",
    "print(df[['PSRD_ICVNC_RGN_NM', 'ì •ì œëœì£¼ì†Œ']].head(10))\n",
    "print(f\"ğŸ“‚ ì •ì œëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def doro_to_coords(df):\n",
    "    data = df.copy()\n",
    "    data['ê²½ë„'] = 0\n",
    "    data['ìœ„ë„'] = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if pd.notna(data.loc[i, 'ì •ì œëœì£¼ì†Œ']) and data.loc[i, 'ì •ì œëœì£¼ì†Œ'] != 'íƒˆí‡´':  # 'ë„ë¡œëª…ì£¼ì†Œ' â†’ 'ì£¼ì†Œ' ë³€ê²½\n",
    "            try:\n",
    "                address = data.loc[i, 'ì •ì œëœì£¼ì†Œ']\n",
    "                url = f'https://dapi.kakao.com/v2/local/search/address.json?query={address}'\n",
    "                headers = {\"Authorization\": \"KakaoAK \" + \"39aa05f286da07187ee24e01563aa0f2\"}\n",
    "                response = requests.get(url, headers=headers).json()\n",
    "                \n",
    "                if response.get('documents'):\n",
    "                    temp = response['documents'][0].get('road_address')  # ë„ë¡œëª…ì£¼ì†Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "                    if temp:\n",
    "                        data.loc[i, ['ê²½ë„', 'ìœ„ë„']] = [temp['x'], temp['y']]\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"ì˜¤ë¥˜ ë°œìƒ: {e} (í–‰ {i})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# CSV íŒŒì¼ ì½ê¸° (ê²½ë¡œ ìˆ˜ì •)\n",
    "# ğŸ”¹ ì—‘ì…€ íŒŒì¼ ì½ê¸° (ê²½ë¡œ ìˆ˜ì •)\n",
    "df = pd.read_excel(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œí†µí–‰ë¶ˆí¸ì§€ì—­_ì •ì œ.xlsx\")\n",
    "\n",
    "\n",
    "# í•¨ìˆ˜ ì ìš©\n",
    "result_df = doro_to_coords(df)\n",
    "\n",
    "result_df.to_csv(\n",
    "    r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œí†µí–‰ë¶ˆí¸ì§€ì—­_ì •ì œ_ìœ„ê²½ë„.csv\", \n",
    "    encoding=\"utf-8-sig\", \n",
    "    index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ„ê²½ë„ê°€ (0, 0)ì¸ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
    "zero_coords_count = ((result_df[\"ìœ„ë„\"] == 0) & (result_df[\"ê²½ë„\"] == 0)).sum()\n",
    "\n",
    "# ì „ì²´ ë°ì´í„° ê°œìˆ˜\n",
    "total_count = len(result_df)\n",
    "\n",
    "# ë¹„ìœ¨ ê³„ì‚°\n",
    "zero_coords_ratio = zero_coords_count / total_count * 100  # í¼ì„¼íŠ¸(%)\n",
    "\n",
    "print(f\"ìœ„ê²½ë„ê°€ (0,0)ìœ¼ë¡œ ë³€í™˜ëœ ë°ì´í„° ê°œìˆ˜: {zero_coords_count}ê°œ\")\n",
    "print(f\"ì „ì²´ ë°ì´í„° ëŒ€ë¹„ ë¹„ìœ¨: {zero_coords_ratio:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì§€í•˜ì²  ìŠ¹í•˜ì°¨ ë°ì´í„° ì „ì²˜ë¦¬ ë° ìœ„ê²½ë„ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Excel íŒŒì¼ ê²½ë¡œ\n",
    "file_path = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì „ì²´_ë„ì‹œì² ë„ì—­ì‚¬ì •ë³´_20241230.xlsx\"\n",
    "\n",
    "# Excel íŒŒì¼ ì½ê¸°\n",
    "info = pd.read_excel(file_path)\n",
    "\n",
    "# ì›í•˜ëŠ” ì¹¼ëŸ¼ë§Œ ì„ íƒ\n",
    "selected_columns = ['ì—­ì‚¬ëª…', 'ë…¸ì„ ëª…', 'ì—­ì‚¬ë„ë¡œëª…ì£¼ì†Œ']\n",
    "\n",
    "# í•´ë‹¹ ì¹¼ëŸ¼ë§Œ ì¶”ì¶œ\n",
    "info = info[selected_columns]\n",
    "\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "print(info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ì—­ì‚¬ë„ë¡œëª…ì£¼ì†Œ'ì—ì„œ ë¶ˆí•„ìš”í•œ ì •ë³´ ì²˜ë¦¬\n",
    "def clean_address(address):\n",
    "    try:\n",
    "        # ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì œê±°í•˜ê³  'ì§€í•˜' ì œê±°\n",
    "        cleaned_address = address.split('(')[0].replace('ì§€í•˜', '')\n",
    "        return cleaned_address\n",
    "    except Exception as e:\n",
    "        return address  # ì˜ˆì™¸ ë°œìƒ ì‹œ ì›ë˜ ì£¼ì†Œ ë°˜í™˜\n",
    "\n",
    "# 'ì—­ì‚¬ë„ë¡œëª…ì£¼ì†Œ' ì¹¼ëŸ¼ì— ì ìš©\n",
    "info['ë„ë¡œëª…ì£¼ì†Œ'] = info['ì—­ì‚¬ë„ë¡œëª…ì£¼ì†Œ'].apply(clean_address)\n",
    "\n",
    "# 'ì—­ì‚¬ë„ë¡œëª…ì£¼ì†Œ' ì¹¼ëŸ¼ ì‚­ì œ\n",
    "info = info.drop(columns=['ì—­ì‚¬ë„ë¡œëª…ì£¼ì†Œ'])\n",
    "\n",
    "# 'ì—­ì‚¬ëª…' ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "info = info.drop_duplicates(subset=['ì—­ì‚¬ëª…'], keep='first')\n",
    "\n",
    "info[\"ì—­ì‚¬ëª…\"] = info[\"ì—­ì‚¬ëª…\"].apply(lambda x: x[:-1] if x.endswith(\"ì—­\") and x != \"ì„œìš¸ì—­\" else x)\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "print(info.head())\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ í™•ì¸ => ì—†ìŒ.\n",
    "info.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "info=info[info[\"ë„ë¡œëª…ì£¼ì†Œ\"].str.contains(\"ê°•ë‚¨êµ¬|ë§ˆí¬êµ¬|ì†¡íŒŒêµ¬\", na=False)]\n",
    "print(info.head(10))\n",
    "print()\n",
    "\n",
    "info[\"êµ¬\"] = info[\"ë„ë¡œëª…ì£¼ì†Œ\"].str.extract(r\"(ê°•ë‚¨êµ¬|ë§ˆí¬êµ¬|ì†¡íŒŒêµ¬)\")  # ë„ë¡œëª…ì£¼ì†Œì—ì„œ êµ¬ ì´ë¦„ ì¶”ì¶œ\n",
    "counts = info[\"êµ¬\"].value_counts()  # êµ¬ë³„ ë°ì´í„° ê°œìˆ˜ ê³„ì‚°\n",
    "print(counts)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV íŒŒì¼ ê²½ë¡œ\n",
    "file_path2 = r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì§€í•˜ì²  12ì›” ìŠ¹í•˜ì°¨.csv\"\n",
    "\n",
    "# ë‘ ë²ˆì§¸ í–‰ë¶€í„° ë¶ˆëŸ¬ì˜¤ê¸° (ì²« ë²ˆì§¸ í–‰ì€ ë°ì´í„°ë¡œ ì²˜ë¦¬)\n",
    "subway = pd.read_csv(file_path2, skiprows=1, header=None)\n",
    "\n",
    "subway.columns = ['ì‚¬ìš©ì¼ì', 'ë…¸ì„ ëª…', 'ì—­ëª…', 'ìŠ¹ì°¨ì´ìŠ¹ê°ìˆ˜', 'í•˜ì°¨ì´ìŠ¹ê°ìˆ˜', 'ë“±ë¡ì¼ì', 'ë¶ˆí•„ìš”í•œì¹¼ëŸ¼']\n",
    "\n",
    "# 'ë“±ë¡ì¼ì'ì™€ 'ë¶ˆí•„ìš”í•œì¹¼ëŸ¼' ì¹¼ëŸ¼ ì‚­ì œ\n",
    "subway = subway.drop(columns=['ë“±ë¡ì¼ì', 'ë¶ˆí•„ìš”í•œì¹¼ëŸ¼'])\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "print(subway.head())\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì—†ìŒ.\n",
    "subway.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©ì¼ìë³„ ê°œìˆ˜ í™•ì¸ & ì •ë ¬\n",
    "\n",
    "usage_counts = subway[\"ì‚¬ìš©ì¼ì\"].value_counts().sort_index() \n",
    "print(usage_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ë…¸ì„ ëª… + ì—­ëª… ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ê°œìˆ˜ ì„¸ê¸°\n",
    "station_counts = subway.groupby([\"ë…¸ì„ ëª…\", \"ì—­ëª…\"]).size().reset_index(name=\"ê±´ìˆ˜\")\n",
    "\n",
    "# ê°œìˆ˜ê°€ 31ì´ ì•„ë‹Œ ë°ì´í„° í•„í„°ë§\n",
    "incomplete_stations = station_counts[station_counts[\"ê±´ìˆ˜\"] != 31]\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(incomplete_stations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°œìˆ˜ê°€ 31ì´ ì•„ë‹Œ ë…¸ì„ ëª… + ì—­ëª… ëª©ë¡ ì¶”ì¶œ\n",
    "drop_stations = set(zip(incomplete_stations[\"ë…¸ì„ ëª…\"], incomplete_stations[\"ì—­ëª…\"]))\n",
    "\n",
    "# ê°œìˆ˜ê°€ 31ì¸ ë°ì´í„°ë§Œ ë‚¨ê¸°ê¸°\n",
    "subw = subway[~subway[[\"ë…¸ì„ ëª…\", \"ì—­ëª…\"]].apply(tuple, axis=1).isin(drop_stations)]\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(subw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë…¸ì„ ëª… + ì—­ëª… ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ê°œìˆ˜ ì„¸ê¸°\n",
    "station_ = subw.groupby([\"ë…¸ì„ ëª…\", \"ì—­ëª…\"]).size().reset_index(name=\"ê±´ìˆ˜\")\n",
    "print(station_)\n",
    "print()\n",
    "\n",
    "# ê°œìˆ˜ê°€ 31ì´ ì•„ë‹Œ ë°ì´í„° í•„í„°ë§\n",
    "incomplete_ = station_[station_[\"ê±´ìˆ˜\"] != 31]\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(incomplete_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©ì¼ìë³„ ê°œìˆ˜ í™•ì¸ & ì •ë ¬ => 611ê°œë¡œ í†µì¼\n",
    "\n",
    "usage_ = subw[\"ì‚¬ìš©ì¼ì\"].value_counts().sort_index() \n",
    "print(usage_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©ì¼ìë³„, ì—­ëª…ë³„ ìŠ¹ì°¨/í•˜ì°¨ ì´í•© ê³„ì‚°\n",
    "subw = subw.groupby(['ì—­ëª…','ë…¸ì„ ëª…'], as_index=False).sum(numeric_only=True)\n",
    "subw= subw.drop(columns=['ì‚¬ìš©ì¼ì'])\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "subw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë°ì´í„° ë“œë””ì–´ ë¬¶ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info.head())\n",
    "print()\n",
    "print(subw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "subw = subw.merge(info[[\"ì—­ì‚¬ëª…\",\"ë„ë¡œëª…ì£¼ì†Œ\"]],  \n",
    "                                              left_on=[\"ì—­ëª…\"],   # subw ê¸°ì¤€\n",
    "                                              right_on=[\"ì—­ì‚¬ëª…\"],  # info ê¸°ì¤€\n",
    "                                              how=\"left\")\n",
    "\n",
    "# ğŸ”¹ ë¶ˆí•„ìš”í•œ 'ì—­ì‚¬ëª…' ì»¬ëŸ¼ ì‚­ì œ\n",
    "subw = subw.drop(columns=[\"ì—­ì‚¬ëª…\"])\n",
    "# 'ì—­ëª…'ê³¼ 'ë…¸ì„ ëª…' ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "subw = subw.drop_duplicates(subset=['ì—­ëª…', 'ë…¸ì„ ëª…'], keep='first')\n",
    "print(subw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subw.isna().sum())\n",
    "print()\n",
    "# 20241201ì¼ìì—ì„œ ë„ë¡œëª…ì£¼ì†Œê°€ ê²°ì¸¡ì¹˜ì¸ ì—­ëª… ì°¾ê¸°\n",
    "missing_address_stations = subw[subw['ë„ë¡œëª…ì£¼ì†Œ'].isnull()]\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ì¸ ì—­ëª…ë“¤ë§Œ í™•ì¸ => ì´ 11ê°œ ê²°ì¸¡ì¹˜\n",
    "print(missing_address_stations[['ì—­ëª…','ë…¸ì„ ëª…']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'ë„ë¡œëª…ì£¼ì†Œ'ì—ì„œ 'ì„œìš¸'ì´ í¬í•¨ë˜ì§€ ì•Šì€ ë°ì´í„° ì œê±°\n",
    "subw_filtered = subw[subw[\"ë„ë¡œëª…ì£¼ì†Œ\"].str.contains(\"ì„œìš¸\", na=False)]\n",
    "\n",
    "# ê°•ë‚¨êµ¬, ë§ˆí¬êµ¬, ì†¡íŒŒêµ¬ ë°ì´í„°ë§Œ ë‚¨ê¸°ê¸°\n",
    "final = subw_filtered[subw_filtered[\"ë„ë¡œëª…ì£¼ì†Œ\"].str.contains(\"ê°•ë‚¨êµ¬|ë§ˆí¬êµ¬|ì†¡íŒŒêµ¬\")]\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(final)\n",
    "\n",
    "final.to_csv(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ ì§€í•˜ì²  ìŠ¹í•˜ì°¨ ì „ì²˜ë¦¬.csv\", encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ë„ë¡œëª…ì£¼ì†Œë¥¼ ìœ„ê²½ë„ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def doro_to_coords(df):\n",
    "    data = df.copy()\n",
    "    data['ê²½ë„'] = 0\n",
    "    data['ìœ„ë„'] = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if pd.notna(data.loc[i, 'ë„ë¡œëª…ì£¼ì†Œ']) and data.loc[i, 'ë„ë¡œëª…ì£¼ì†Œ'] != 'íƒˆí‡´':  # 'ë„ë¡œëª…ì£¼ì†Œ' â†’ 'ì£¼ì†Œ' ë³€ê²½\n",
    "            try:\n",
    "                address = data.loc[i, 'ë„ë¡œëª…ì£¼ì†Œ']\n",
    "                url = f'https://dapi.kakao.com/v2/local/search/address.json?query={address}'\n",
    "                headers = {\"Authorization\": \"KakaoAK \" + \"39aa05f286da07187ee24e01563aa0f2\"}  # ë³¸ì¸ì˜ Kakao API í‚¤\n",
    "                response = requests.get(url, headers=headers).json()\n",
    "                \n",
    "                if response.get('documents'):\n",
    "                    temp = response['documents'][0].get('road_address')  # ë„ë¡œëª…ì£¼ì†Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "                    if temp:\n",
    "                        data.loc[i, ['ê²½ë„', 'ìœ„ë„']] = [temp['x'], temp['y']]\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"ì˜¤ë¥˜ ë°œìƒ: {e} (í–‰ {i})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# CSV íŒŒì¼ ì½ê¸° (ê²½ë¡œ ìˆ˜ì •)\n",
    "df = pd.read_csv(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì„œìš¸ì‹œ ì§€í•˜ì²  ìŠ¹í•˜ì°¨ ì „ì²˜ë¦¬.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# í•¨ìˆ˜ ì ìš©\n",
    "result_df = doro_to_coords(df)\n",
    "\n",
    "# ê° êµ¬ë³„ë¡œ ë°ì´í„° ë‚˜ëˆ„ê¸°\n",
    "gangnam = result_df[result_df['ë„ë¡œëª…ì£¼ì†Œ'].str.contains(\"ê°•ë‚¨êµ¬\", na=False)]\n",
    "mapo = result_df[result_df['ë„ë¡œëª…ì£¼ì†Œ'].str.contains(\"ë§ˆí¬êµ¬\", na=False)]\n",
    "songpa = result_df[result_df['ë„ë¡œëª…ì£¼ì†Œ'].str.contains(\"ì†¡íŒŒêµ¬\", na=False)]\n",
    "\n",
    "# ê° êµ¬ë³„ë¡œ ìœ„ê²½ë„ê°€ 0ì¸ ë°ì´í„° ê°œìˆ˜ì™€ ë¹„ìœ¨ ê³„ì‚°\n",
    "def calculate_missing_coords(data):\n",
    "    missing_coords = data[(data['ê²½ë„'] == 0) & (data['ìœ„ë„'] == 0)]\n",
    "    missing_count = len(missing_coords)\n",
    "    total_count = len(data)\n",
    "    missing_ratio = missing_count / total_count * 100  # ë¹„ìœ¨ ê³„ì‚°\n",
    "    return missing_count, missing_ratio\n",
    "\n",
    "# ê°•ë‚¨êµ¬, ë§ˆí¬êµ¬, ì†¡íŒŒêµ¬ì˜ ê²°ì¸¡ì¹˜ ê°œìˆ˜ì™€ ë¹„ìœ¨ í™•ì¸\n",
    "gangnam_missing_count, gangnam_missing_ratio = calculate_missing_coords(gangnam)\n",
    "mapo_missing_count, mapo_missing_ratio = calculate_missing_coords(mapo)\n",
    "songpa_missing_count, songpa_missing_ratio = calculate_missing_coords(songpa)\n",
    "\n",
    "print(f\"ê°•ë‚¨êµ¬ ê²°ì¸¡ì¹˜ ê°œìˆ˜: {gangnam_missing_count}, ë¹„ìœ¨: {gangnam_missing_ratio:.2f}%\")\n",
    "print(f\"ë§ˆí¬êµ¬ ê²°ì¸¡ì¹˜ ê°œìˆ˜: {mapo_missing_count}, ë¹„ìœ¨: {mapo_missing_ratio:.2f}%\")\n",
    "print(f\"ì†¡íŒŒêµ¬ ê²°ì¸¡ì¹˜ ê°œìˆ˜: {songpa_missing_count}, ë¹„ìœ¨: {songpa_missing_ratio:.2f}%\")\n",
    "\n",
    "# ê° êµ¬ë³„ë¡œ íŒŒì¼ ì €ì¥\n",
    "gangnam.to_csv(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ê°•ë‚¨êµ¬_ì„œìš¸ì‹œ ì§€í•˜ì²  ìŠ¹í•˜ì°¨ ìœ„ê²½ë„.csv\", encoding='utf-8-sig', index=False)\n",
    "mapo.to_csv(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ë§ˆí¬êµ¬_ì„œìš¸ì‹œ ì§€í•˜ì²  ìŠ¹í•˜ì°¨ ìœ„ê²½ë„.csv\", encoding='utf-8-sig', index=False)\n",
    "songpa.to_csv(r\"C:\\Users\\user\\Desktop\\0221 ë¯¸íŒ…\\ì†¡íŒŒêµ¬_ì„œìš¸ì‹œ ì§€í•˜ì²  ìŠ¹í•˜ì°¨ ìœ„ê²½ë„.csv\", encoding='utf-8-sig', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
